apiVersion: apps/v1

kind: Deployment

metadata:

  name: dnstools

spec:

  replicas: 1

  selector:

    matchLabels:

      app: dnstools

  template:

    metadata:

      labels:

        app: dnstools

    spec:

      containers:

      - name: dnstools

        image: infoblox/dnstools

        command: [ "sleep", "3600000"]

        tty: true


nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b
1.5.2
deepseek-r1-distill-llama-8b


curl -X POST http://nim-llm.rag.svc.cluster.local:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-ai/deepseek-r1-distill-llama-8b",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'



frontend:
  enabled: true
  service:
    type: NodePort
    port: 3000
  envVars:
    - name: NEXT_PUBLIC_MODEL_NAME
      value: "deepseek-ai/deepseek-r1-distill-llama-8b"


  nv-ingest:
    imagePullSecrets:
      - name: "ngc-secret"
    ngcApiSecret:
      create: false
    ngcImagePullSecret:
      create: false
    image:
      repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
      tag: "latest"


nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b

helm upgrade --install rag -n rag https://helm.ngc.nvidia.com/nvidia/blueprint/charts/nvidia-blueprint-rag-v2.2.0.tgz \
  --username '$oauthtoken' \
  --password "${NGC_API_KEY}" \
  --set imagePullSecret.password=$NGC_API_KEY \
  --set ngcApiSecret.password=$NGC_API_KEY \
  --set ingestor-server.envVars.APP_VECTORSTORE_ENABLEGPUINDEX=False \
  --set ingestor-server.envVars.APP_VECTORSTORE_ENABLEGPUSEARCH=False \
  -f /tony/mig_values.yaml 

export NGC_API_KEY="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"


apiVersion: apps.nvidia.com/v1alpha1
kind: NIMCache
metadata:
  name: deepseek-r1-distill-llama-8b
  namespace: nim-service
spec:
  source:
    ngc:
      modelPuller: nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b:1.5.2
      pullSecret: ngc-secret
      authSecret: ngc-api-secret
      model:
        engine: tensorrt_llm
        tensorParallelism: "1"
  storage:
    pvc:
      create: true
      size: "30Gi"
      volumeAccessMode: ReadWriteMany
  resources: {}

#Check status
 kubectl get nimcaches.apps.nvidia.com -n nim-service
NAME                           STATUS       PVC   AGE
deepseek-r1-distill-llama-8b   InProgress         4m55s


apiVersion: apps.nvidia.com/v1alpha1
kind: NIMService
metadata:
  name: deepseek-r1-distill-llama-8b
spec:
  source:
    ngc:
      modelPuller: nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b:1.5.2
      pullSecret: ngc-secret
      authSecret: ngc-api-secret
      model:   #Include the model object to describe the LLM-specific model you want to pull from NGC
        engine: tensorrt_llm
        tensorParallelism: "1"
  storage:
    nimCache:
      name: deepseek-r1-distill-llama-8b
      profile: ''
  resources:
    limits:
      nvidia.com/mig-7g.40gb: 1
  expose:
    service:
      type: ClusterIP
      port: 8000



kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: minio-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: nfs-client-pokprod
---

kind: Secret
apiVersion: v1
metadata:
  name: minio-secret
stringData:
  minio_root_user: minio
  minio_root_password: minio123
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: minio
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: minio
    spec:
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: minio-pvc
      containers:
        - resources:
            limits:
              cpu: 250m
              memory: 1Gi
            requests:
              cpu: 20m
              memory: 100Mi
          readinessProbe:
            tcpSocket:
              port: 9000
            initialDelaySeconds: 5
            timeoutSeconds: 1
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          terminationMessagePath: /dev/termination-log
          name: minio
          livenessProbe:
            tcpSocket:
              port: 9000
            initialDelaySeconds: 30
            timeoutSeconds: 1
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: minio_root_user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio-secret
                  key: minio_root_password
          ports:
            - containerPort: 9000
              protocol: TCP
            - containerPort: 9090
              protocol: TCP
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: data
              mountPath: /data
              subPath: minio
          terminationMessagePolicy: File
          image: >-
            quay.io/minio/minio:RELEASE.2025-07-18T21-56-31Z
          args:
            - server
            - /data
            - --console-address
            - :9090
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
---
kind: Service
apiVersion: v1
metadata:
  name: minio-service
spec:
  ipFamilies:
    - IPv4
  ports:
    - name: api
      protocol: TCP
      port: 9000
      targetPort: 9000
    - name: ui
      protocol: TCP
      port: 9090
      targetPort: 9090
  internalTrafficPolicy: Cluster
  type: ClusterIP
  ipFamilyPolicy: SingleStack
  sessionAffinity: None
  selector:
    app: minio
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: minio-api
spec:
  to:
    kind: Service
    name: minio-service
    weight: 100
  port:
    targetPort: api
  wildcardPolicy: None
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: minio-ui
spec:
  to:
    kind: Service
    name: minio-service
    weight: 100
  port:
    targetPort: ui
  wildcardPolicy: None
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

http://gemma3-predictor.antonio.svc.cluster.local

curl -X POST http://gemma3-predictor.antonio.svc.cluster.local/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'

curl http://llama-predictor.nim-project.svc.cluster.local/v1/models

curl -X POST http://llama-predictor.nim-project.svc.cluster.local/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-ai/deepseek-r1-distill-llama-8b",
    "prompt": "what is kubernetes",
    "temperature": 0.7,
    "max_tokens": 200,
    "stop": null
  }'

curl -X POST http://gemma3-predictor.antonio.svc.cluster.local:8000/v1/models
curl https://gemma3-antonio.apps.barolo.skynet.ai/v1/models

curl --insecure -X POST https://gemma3-antonio.apps.barolo.skynet.ai/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'

curl --insecure -X POST https://gemma3-antonio.apps.barolo.skynet.ai/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'

--max-model-len=4096
--max-num-seqs=48
--tensor-parallel-size=2

--quantization="awq_marlin"
--enable-auto-tool-choice
--tool-call-parser="hermes"
--trust-remote-code


--max-model-len=65536
--rope-scaling={"rope_type":"yarn","factor":2.0,"original_max_position_embeddings":32768} 
--max-num-batched-tokens=16384
--max-num-seqs=32

oc create secret -n antonio docker-registry ngc-secret --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password=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



oc create secret -n antonio generic ngc-api-secret --from-literal=NGC_API_KEY=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

curl -X POST http://deepseek-r1-distill-llama-8b:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-ai/deepseek-r1-distill-llama-8b",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'

apiVersion: batch/v1
kind: Job
metadata:
  name: nim-profile-job
spec:
  template:
    metadata:
      name: nim-profile-pod
    spec:
      containers:
      - name: nim-profile
        # Update the image name to the NIM that will be deployed in production
        image: nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b
        args: ["list-model-profiles"]
        env:
        - name: NIM_CACHE_PATH
          value: /tmp
        - name: NGC_API_KEY
          value: "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"   
      imagePullSecrets:
      - name: ngc-secret
      restartPolicy: Never

c rsh -n nvidia-gpu-operator $(oc -n nvidia-gpu-operator get pod -o name -l app.kubernetes.io/component=nvidia-driver)

oc describe node -l node-role.kubernetes.io/worker=| grep -E 'Capacity:|Allocatable:'

oc process vllm-multinode-runtime-template -n redhat-ods-applications|oc apply -n antonio -f -

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen32multi
  namespace: antonio
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/autoscalerClass: external
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-multinode-runtime
      storageUri: pvc://model-cache-pvc/qwen/qwen3-32b
    workerSpec: 
      tensorParallelSize: 1
      pipelineParallelSize: 2

curl qwen32multi-predictor.antonio.svc.cluster.local/v1/models
curl -X POST http://qwen32multi-predictor.antonio.svc.cluster.local/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen32multi",
    "prompt": "Explain the theory of relativity in simple terms.",
    "temperature": 0.7,
    "max_tokens": 512,
    "stop": null
  }'


  - model_name: qwen-32b
    litellm_params: 
      model: hosted_vllm/Qwen/Qwen3-32b
      api_base: http://vllm-qwen-32b-service.antonio.svc.cluster.local:8000/v1
  - model_name: qwen-32b-multi
    litellm_params: 
      model: hosted_vllm/qwen32multi
      api_base: http://qwen32multi-predictor.antonio.svc.cluster.local/v1
  - model_name: gemma-yarn
    litellm_params: 
      model: hosted_vllm/gemmayarn
      api_base: http://gemmayarn-predictor.antonio.svc.cluster.local/v1
  - model_name: deepseej

curl -X POST http://litellm.antonio.svc.cluster.local:4000/v1/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-1234" \
  -d '{
    "model": "deepseek",
    "prompt": "What is Kubernetes?.",
    "temperature": 0.7,
    "max_tokens": 100,
    "stop": null
  }'
